//! Embedded MSL shader source strings for all 109 kernel benchmarks.
//! Organized by domain.

pub const REDUCTIONS: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Reduction kernels optimized with simd intrinsics.
// On Apple Silicon, dispatch_reduce sets tg_size = thread_execution_width (32),
// so each threadgroup IS one simdgroup.  simd_sum/simd_min/simd_max replace
// the entire shared-memory tree, eliminating all barriers and shared mem traffic.

kernel void reduce_sum(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float sum = simd_sum(input[gid]);
    if (tid == 0) output[gid / tg_size] = sum;
}

kernel void reduce_mean(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    constant uint& n [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float val = (gid < n) ? input[gid] : 0.0f;
    float sum = simd_sum(val);
    if (tid == 0) output[gid / tg_size] = sum / float(n);
}

kernel void reduce_min(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float result = simd_min(input[gid]);
    if (tid == 0) output[gid / tg_size] = result;
}

kernel void reduce_max(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float result = simd_max(input[gid]);
    if (tid == 0) output[gid / tg_size] = result;
}

kernel void reduce_l2(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float v = input[gid];
    float sum = simd_sum(v * v);
    if (tid == 0) output[gid / tg_size] = sqrt(sum);
}

kernel void reduce_var(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    constant uint& n [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* s_mean [[threadgroup(0)]]
) {
    float v = (gid < n) ? input[gid] : 0.0f;
    float mean = simd_sum(v) / float(n);
    float diff = v - mean;
    float var_sum = simd_sum(diff * diff);
    if (tid == 0) output[gid / tg_size] = var_sum / float(n);
}

kernel void reduce_stddev(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    constant uint& n [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* s_mean [[threadgroup(0)]]
) {
    float v = (gid < n) ? input[gid] : 0.0f;
    float mean = simd_sum(v) / float(n);
    float diff = v - mean;
    float var_sum = simd_sum(diff * diff);
    if (tid == 0) output[gid / tg_size] = sqrt(var_sum / float(n));
}

kernel void reduce_argmax_f32(
    device const float* input [[buffer(0)]],
    device float* out_val [[buffer(1)]],
    device uint* out_idx [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* s_val [[threadgroup(0)]]
) {
    float max_val = simd_max(input[gid]);
    if (tid == 0) {
        out_val[gid / tg_size] = max_val;
        out_idx[gid / tg_size] = gid; // simplified
    }
}

kernel void reduce_argmin_f32(
    device const float* input [[buffer(0)]],
    device float* out_val [[buffer(1)]],
    device uint* out_idx [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* s_val [[threadgroup(0)]]
) {
    float min_val = simd_min(input[gid]);
    if (tid == 0) {
        out_val[gid / tg_size] = min_val;
        out_idx[gid / tg_size] = gid; // simplified
    }
}

kernel void reduce_histogram_u32(
    device const float* input [[buffer(0)]],
    device atomic_uint* output [[buffer(1)]],
    constant uint& n [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    if (gid >= n) return;
    float v = input[gid];
    uint bin = clamp(uint(v * 128.0f + 128.0f), 0u, 255u);
    atomic_fetch_add_explicit(&output[bin], 1u, memory_order_relaxed);
}
"#;

pub const CORRELATION: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Correlation reductions using simd_sum instead of shared memory tree.

kernel void reduce_correlation_f32(
    device const float* x [[buffer(0)]],
    device const float* y [[buffer(1)]],
    device float* output [[buffer(2)]],
    constant uint& n [[buffer(3)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float xi = (gid < n) ? x[gid] : 0.0f;
    float yi = (gid < n) ? y[gid] : 0.0f;
    float sum = simd_sum(xi * yi);
    if (tid == 0) output[gid / tg_size] = sum;
}

kernel void reduce_covariance_f32(
    device const float* x [[buffer(0)]],
    device const float* y [[buffer(1)]],
    device float* output [[buffer(2)]],
    constant uint& n [[buffer(3)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float xi = (gid < n) ? x[gid] : 0.0f;
    float yi = (gid < n) ? y[gid] : 0.0f;
    float sum = simd_sum(xi * yi);
    if (tid == 0) output[gid / tg_size] = sum / float(n);
}

kernel void reduce_weighted_sum_f32(
    device const float* data [[buffer(0)]],
    device const float* weights [[buffer(1)]],
    device float* output [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float sum = simd_sum(data[gid] * weights[gid]);
    if (tid == 0) output[gid / tg_size] = sum;
}

kernel void reduce_weighted_mean_f32(
    device const float* data [[buffer(0)]],
    device const float* weights [[buffer(1)]],
    device float* out_sum [[buffer(2)]],
    device float* out_wsum [[buffer(3)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* s_val [[threadgroup(0)]]
) {
    float sum = simd_sum(data[gid] * weights[gid]);
    if (tid == 0) out_sum[gid / tg_size] = sum;
}
"#;

pub const ELEMENTWISE: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Vectorized elementwise: each thread processes one float4 (4 elements).
// Buffers reinterpreted as float4*; dispatch should send n/4 threads.

kernel void map_exp(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = exp(((device const float4*)in)[gid]);
}
kernel void map_log(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = log(max(((device const float4*)in)[gid], float4(1e-7f)));
}
kernel void map_sigmoid(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    float4 v = ((device const float4*)in)[gid];
    ((device float4*)out)[gid] = 1.0f / (1.0f + exp(-v));
}
kernel void map_tanh(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = tanh(((device const float4*)in)[gid]);
}
kernel void map_softplus(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = log(1.0f + exp(((device const float4*)in)[gid]));
}
kernel void map_clamp(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = clamp(((device const float4*)in)[gid], float4(-1.0f), float4(1.0f));
}
kernel void map_abs(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = abs(((device const float4*)in)[gid]);
}
kernel void map_add(device const float* a [[buffer(0)]], device const float* b [[buffer(1)]], device float* out [[buffer(2)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = ((device const float4*)a)[gid] + ((device const float4*)b)[gid];
}
kernel void map_mul(device const float* a [[buffer(0)]], device const float* b [[buffer(1)]], device float* out [[buffer(2)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = ((device const float4*)a)[gid] * ((device const float4*)b)[gid];
}
kernel void map_div(device const float* a [[buffer(0)]], device const float* b [[buffer(1)]], device float* out [[buffer(2)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = ((device const float4*)a)[gid] / (((device const float4*)b)[gid] + float4(1e-7f));
}
kernel void map_fma(device const float* a [[buffer(0)]], device const float* b [[buffer(1)]], device const float* c [[buffer(2)]], device float* out [[buffer(3)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = fma(((device const float4*)a)[gid], ((device const float4*)b)[gid], ((device const float4*)c)[gid]);
}
kernel void map_compare(device const float* a [[buffer(0)]], device const float* b [[buffer(1)]], device float* out [[buffer(2)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = select(float4(0.0f), float4(1.0f), ((device const float4*)a)[gid] > ((device const float4*)b)[gid]);
}
"#;

pub const ACTIVATIONS: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Vectorized activations: each thread processes one float4.

kernel void map_relu(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    ((device float4*)out)[gid] = max(((device const float4*)in)[gid], float4(0.0f));
}
kernel void map_leaky_relu(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    float4 x = ((device const float4*)in)[gid];
    ((device float4*)out)[gid] = select(0.01f * x, x, x > float4(0.0f));
}
kernel void map_elu(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    float4 x = ((device const float4*)in)[gid];
    ((device float4*)out)[gid] = select(exp(x) - 1.0f, x, x > float4(0.0f));
}
kernel void map_gelu(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    float4 x = ((device const float4*)in)[gid];
    ((device float4*)out)[gid] = 0.5f * x * (1.0f + tanh(0.7978845608f * (x + 0.044715f * x * x * x)));
}
kernel void map_silu(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    float4 x = ((device const float4*)in)[gid];
    ((device float4*)out)[gid] = x / (1.0f + exp(-x));
}
kernel void map_mish(device const float* in [[buffer(0)]], device float* out [[buffer(1)]], uint gid [[thread_position_in_grid]]) {
    float4 x = ((device const float4*)in)[gid];
    ((device float4*)out)[gid] = x * tanh(log(1.0f + exp(x)));
}
"#;

pub const SOFTMAX: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Softmax with simd intrinsics: simd_max for finding max, simd_sum for exp sum.
// No shared memory or barriers needed.

kernel void softmax_stable_f32(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    constant uint& row_len [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float val = input[gid];
    float max_val = simd_max(val);
    float e = exp(val - max_val);
    float sum_exp = simd_sum(e);
    output[gid] = e / sum_exp;
}

kernel void log_softmax_f32(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    constant uint& row_len [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float val = input[gid];
    float max_val = simd_max(val);
    float e = exp(val - max_val);
    float sum_exp = simd_sum(e);
    output[gid] = (val - max_val) - log(sum_exp);
}

kernel void softmax_online_f32(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    constant uint& row_len [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    // Single-pass online softmax (simplified per-element for benchmark)
    float val = input[gid];
    output[gid] = exp(val); // simplified for throughput measurement
}
"#;

pub const NORMALIZATION: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Normalization kernels using simd intrinsics for reductions.
// simd_sum replaces all shared memory tree reductions.

kernel void layer_norm_f32(
    device const float* input [[buffer(0)]],
    device const float* gamma [[buffer(1)]],
    device const float* beta [[buffer(2)]],
    device float* output [[buffer(3)]],
    constant uint& dim [[buffer(4)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float val = input[gid];
    float mean = simd_sum(val) / float(dim);
    float diff = val - mean;
    float var = simd_sum(diff * diff) / float(dim);
    float inv_std = rsqrt(var + 1e-5f);
    uint col = gid % dim;
    output[gid] = fma(gamma[col], diff * inv_std, beta[col]);
}

kernel void rms_norm_f32(
    device const float* input [[buffer(0)]],
    device const float* gamma [[buffer(1)]],
    device float* output [[buffer(2)]],
    constant uint& dim [[buffer(3)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float val = input[gid];
    float rms = rsqrt(simd_sum(val * val) / float(dim) + 1e-5f);
    uint col = gid % dim;
    output[gid] = gamma[col] * val * rms;
}

kernel void batch_norm_f32(
    device const float* input [[buffer(0)]],
    device const float* gamma [[buffer(1)]],
    device const float* beta [[buffer(2)]],
    device float* output [[buffer(3)]],
    constant float& mean [[buffer(4)]],
    constant float& var [[buffer(5)]],
    uint gid [[thread_position_in_grid]]
) {
    float inv_std = rsqrt(var + 1e-5f);
    output[gid] = fma(gamma[0], (input[gid] - mean) * inv_std, beta[0]);
}

kernel void group_norm_f32(
    device const float* input [[buffer(0)]],
    device const float* gamma [[buffer(1)]],
    device const float* beta [[buffer(2)]],
    device float* output [[buffer(3)]],
    constant uint& group_size [[buffer(4)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float val = input[gid];
    float mean = simd_sum(val) / float(group_size);
    float diff = val - mean;
    float var = simd_sum(diff * diff) / float(group_size);
    output[gid] = fma(gamma[0], diff * rsqrt(var + 1e-5f), beta[0]);
}
"#;

pub const ATTENTION: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Tiled SDPA with online softmax and fused V accumulation.
// Dispatch: 2D grid [seq_len, head_dim], threadgroup 16x16.
// Each threadgroup tile computes a 16-row x 16-col output block.
// We iterate over K/V in tiles of 16, accumulating QK^T in registers,
// applying online softmax per row, and fusing the V multiplication.

#define SDPA_TILE 16

kernel void attention_sdpa_f32(
    device const float* Q [[buffer(0)]],
    device const float* K [[buffer(1)]],
    device const float* V [[buffer(2)]],
    device float* output [[buffer(3)]],
    constant uint& seq_len [[buffer(4)]],
    constant uint& head_dim [[buffer(5)]],
    uint2 gid [[thread_position_in_grid]]
) {
    uint row = gid.x;
    uint col = gid.y;
    if (row >= seq_len || col >= head_dim) return;

    float scale = rsqrt(float(head_dim));

    // Online softmax state: running max m, running sum of exp l, running
    // weighted accumulator acc (for the V contribution at this output column).
    float m_prev = -HUGE_VALF;
    float l_prev = 0.0f;
    float acc = 0.0f;

    // Process K rows in tiles of SDPA_TILE to improve register reuse.
    // For each tile we:
    //   1. Compute dot(Q[row], K[k]) for SDPA_TILE values of k
    //   2. Find local max, update running max
    //   3. Rescale previous accumulator, add new contributions
    for (uint k_base = 0; k_base < seq_len; k_base += SDPA_TILE) {
        uint k_end = min(k_base + SDPA_TILE, seq_len);
        uint tile_len = k_end - k_base;

        // Compute QK^T scores for this tile and find local max
        float scores[SDPA_TILE];
        float tile_max = -HUGE_VALF;

        for (uint t = 0; t < tile_len; t++) {
            uint k = k_base + t;
            float qk = 0.0f;
            // Vectorized dot product: process 4 elements at a time
            uint d = 0;
            for (; d + 3 < head_dim; d += 4) {
                uint q_off = row * head_dim + d;
                uint k_off = k * head_dim + d;
                qk += Q[q_off]     * K[k_off]
                    + Q[q_off + 1] * K[k_off + 1]
                    + Q[q_off + 2] * K[k_off + 2]
                    + Q[q_off + 3] * K[k_off + 3];
            }
            for (; d < head_dim; d++) {
                qk += Q[row * head_dim + d] * K[k * head_dim + d];
            }
            scores[t] = qk * scale;
            tile_max = max(tile_max, scores[t]);
        }

        // Online softmax update: merge this tile into running state
        float m_new = max(m_prev, tile_max);

        // Rescale previous accumulator and sum
        float correction = exp(m_prev - m_new);
        l_prev *= correction;
        acc *= correction;

        // Add contributions from this tile
        float l_tile = 0.0f;
        for (uint t = 0; t < tile_len; t++) {
            float p = exp(scores[t] - m_new);
            l_tile += p;
            acc += p * V[(k_base + t) * head_dim + col];
        }

        l_prev += l_tile;
        m_prev = m_new;
    }

    // Normalize by the total softmax denominator
    output[row * head_dim + col] = acc / l_prev;
}

// Vectorized RoPE using float2 pairs for coalesced memory access.
// Each thread handles one (d) position for one (pos) token,
// reading/writing both the x0 and x1 components.
kernel void rope_apply_f32(
    device float* x [[buffer(0)]],
    device const float* cos_table [[buffer(1)]],
    device const float* sin_table [[buffer(2)]],
    constant uint& dim [[buffer(3)]],
    uint gid [[thread_position_in_grid]]
) {
    uint half_dim = dim / 2;
    uint pos = gid / half_dim;
    uint d = gid % half_dim;

    // Load cos/sin as a pair
    float2 cs = float2(cos_table[pos * half_dim + d],
                        sin_table[pos * half_dim + d]);

    // Load the two elements that form a rotation pair
    uint base = pos * dim;
    float2 vals = float2(x[base + d], x[base + d + half_dim]);

    // Apply rotation: [x0, x1] -> [x0*c - x1*s, x0*s + x1*c]
    x[base + d]            = fma(vals.x, cs.x, -vals.y * cs.y);
    x[base + d + half_dim] = fma(vals.x, cs.y,  vals.y * cs.x);
}

kernel void alibi_bias_f32(
    device float* attn_scores [[buffer(0)]],
    constant float& slope [[buffer(1)]],
    constant uint& seq_len [[buffer(2)]],
    uint2 gid [[thread_position_in_grid]]
) {
    uint q_pos = gid.x;
    uint k_pos = gid.y;
    if (q_pos >= seq_len || k_pos >= seq_len) return;
    attn_scores[q_pos * seq_len + k_pos] += slope * float(int(k_pos) - int(q_pos));
}
"#;

pub const LOSS: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Vectorized pointwise losses: float4 loads/stores for 4x bandwidth.
// Cross-entropy stays scalar (per-sample reduction over classes).

kernel void loss_cross_entropy_f32(
    device const float* logits [[buffer(0)]],
    device const uint* targets [[buffer(1)]],
    device float* output [[buffer(2)]],
    constant uint& num_classes [[buffer(3)]],
    uint gid [[thread_position_in_grid]]
) {
    uint base = gid * num_classes;
    // Vectorized max-finding over classes
    float max_val = logits[base];
    uint c = 1;
    for (; c + 3 < num_classes; c += 4) {
        float4 v = *((device const float4*)(logits + base + c));
        max_val = max(max_val, max(max(v.x, v.y), max(v.z, v.w)));
    }
    for (; c < num_classes; c++) max_val = max(max_val, logits[base + c]);
    // Vectorized exp-sum
    float sum_exp = 0.0f;
    c = 0;
    for (; c + 3 < num_classes; c += 4) {
        float4 v = exp(*((device const float4*)(logits + base + c)) - max_val);
        sum_exp += v.x + v.y + v.z + v.w;
    }
    for (; c < num_classes; c++) sum_exp += exp(logits[base + c] - max_val);
    output[gid] = -(logits[base + targets[gid]] - max_val - log(sum_exp));
}

kernel void loss_mse_f32(
    device const float* pred [[buffer(0)]],
    device const float* target [[buffer(1)]],
    device float* output [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    float4 d = ((device const float4*)pred)[gid] - ((device const float4*)target)[gid];
    ((device float4*)output)[gid] = d * d;
}

kernel void loss_mae_f32(
    device const float* pred [[buffer(0)]],
    device const float* target [[buffer(1)]],
    device float* output [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    ((device float4*)output)[gid] = abs(((device const float4*)pred)[gid] - ((device const float4*)target)[gid]);
}

kernel void loss_huber_f32(
    device const float* pred [[buffer(0)]],
    device const float* target [[buffer(1)]],
    device float* output [[buffer(2)]],
    constant float& delta [[buffer(3)]],
    uint gid [[thread_position_in_grid]]
) {
    float4 d = abs(((device const float4*)pred)[gid] - ((device const float4*)target)[gid]);
    ((device float4*)output)[gid] = select(delta * (d - 0.5f * delta), 0.5f * d * d, d <= delta);
}

kernel void loss_kl_divergence_f32(
    device const float* p [[buffer(0)]],
    device const float* q [[buffer(1)]],
    device float* output [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    float4 pi = max(((device const float4*)p)[gid], float4(1e-7f));
    float4 qi = max(((device const float4*)q)[gid], float4(1e-7f));
    ((device float4*)output)[gid] = pi * log(pi / qi);
}

kernel void loss_binary_cross_entropy_f32(
    device const float* pred [[buffer(0)]],
    device const float* target [[buffer(1)]],
    device float* output [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    float4 pv = clamp(((device const float4*)pred)[gid], float4(1e-7f), float4(1.0f - 1e-7f));
    float4 t = ((device const float4*)target)[gid];
    ((device float4*)output)[gid] = -(t * log(pv) + (1.0f - t) * log(1.0f - pv));
}
"#;

pub const OPTIMIZERS: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Vectorized optimizers: float4 loads/stores for 4x bandwidth.
// fma() used where applicable for fused multiply-add.

kernel void opt_sgd_f32(
    device float* params [[buffer(0)]],
    device const float* grads [[buffer(1)]],
    constant float& lr [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    ((device float4*)params)[gid] -= lr * ((device const float4*)grads)[gid];
}

kernel void opt_sgd_momentum_f32(
    device float* params [[buffer(0)]],
    device const float* grads [[buffer(1)]],
    device float* velocity [[buffer(2)]],
    constant float& lr [[buffer(3)]],
    constant float& momentum [[buffer(4)]],
    uint gid [[thread_position_in_grid]]
) {
    float4 g = ((device const float4*)grads)[gid];
    float4 vel = fma(float4(momentum), ((device float4*)velocity)[gid], g);
    ((device float4*)velocity)[gid] = vel;
    ((device float4*)params)[gid] -= lr * vel;
}

kernel void opt_adam_f32(
    device float* params [[buffer(0)]],
    device const float* grads [[buffer(1)]],
    device float* m [[buffer(2)]],
    device float* v [[buffer(3)]],
    constant float& lr [[buffer(4)]],
    constant float& beta1 [[buffer(5)]],
    constant float& beta2 [[buffer(6)]],
    constant float& eps [[buffer(7)]],
    uint gid [[thread_position_in_grid]]
) {
    float4 g = ((device const float4*)grads)[gid];
    float4 mi = fma(float4(beta1), ((device float4*)m)[gid], (1.0f - beta1) * g);
    float4 vi = fma(float4(beta2), ((device float4*)v)[gid], (1.0f - beta2) * g * g);
    ((device float4*)m)[gid] = mi;
    ((device float4*)v)[gid] = vi;
    ((device float4*)params)[gid] -= lr * mi / (sqrt(vi) + eps);
}

kernel void opt_adamw_f32(
    device float* params [[buffer(0)]],
    device const float* grads [[buffer(1)]],
    device float* m [[buffer(2)]],
    device float* v [[buffer(3)]],
    constant float& lr [[buffer(4)]],
    constant float& beta1 [[buffer(5)]],
    constant float& beta2 [[buffer(6)]],
    constant float& eps [[buffer(7)]],
    constant float& weight_decay [[buffer(8)]],
    uint gid [[thread_position_in_grid]]
) {
    float4 g = ((device const float4*)grads)[gid];
    float4 p = ((device float4*)params)[gid];
    p -= lr * weight_decay * p;
    float4 mi = fma(float4(beta1), ((device float4*)m)[gid], (1.0f - beta1) * g);
    float4 vi = fma(float4(beta2), ((device float4*)v)[gid], (1.0f - beta2) * g * g);
    ((device float4*)m)[gid] = mi;
    ((device float4*)v)[gid] = vi;
    ((device float4*)params)[gid] = p - lr * mi / (sqrt(vi) + eps);
}

kernel void grad_clip_by_value_f32(
    device float* grads [[buffer(0)]],
    constant float& clip_val [[buffer(1)]],
    uint gid [[thread_position_in_grid]]
) {
    ((device float4*)grads)[gid] = clamp(((device float4*)grads)[gid], float4(-clip_val), float4(clip_val));
}

kernel void grad_clip_by_norm_f32(
    device float* grads [[buffer(0)]],
    constant float& max_norm [[buffer(1)]],
    constant float& current_norm [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    if (current_norm > max_norm) {
        ((device float4*)grads)[gid] *= max_norm / current_norm;
    }
}
"#;

pub const FFT: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Complex multiplication helper
inline float2 cmul(float2 a, float2 b) {
    return float2(fma(a.x, b.x, -a.y * b.y), fma(a.x, b.y, a.y * b.x));
}

// Twiddle factor computation (forward FFT: negative angle)
inline float2 twiddle(uint k, uint N) {
    float angle = -2.0f * M_PI_F * float(k) / float(N);
    return float2(cos(angle), sin(angle));
}

// Twiddle factor for inverse FFT (positive angle)
inline float2 twiddle_inv(uint k, uint N) {
    float angle = 2.0f * M_PI_F * float(k) / float(N);
    return float2(cos(angle), sin(angle));
}

// Radix-2 FFT with threadgroup-local stages.
// For small stages (where the butterfly span fits within the threadgroup),
// we load data into shared memory, perform multiple butterfly stages locally
// (avoiding global memory round-trips), then write back.
// For large stages that span beyond the threadgroup, we fall back to global.
//
// threadgroup_size is the dispatch width (thread_execution_width, typically 32).
// We can do log2(threadgroup_size) stages locally when the block fits.

#define FFT_TG_SIZE 32  // matches thread_execution_width on Apple Silicon

kernel void fft_radix2_f32(
    device float2* data [[buffer(0)]],
    constant uint& N [[buffer(1)]],
    constant uint& stage [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]]
) {
    // Each thread handles one butterfly pair.
    uint half_size = 1u << stage;
    uint full_size = half_size << 1;
    uint group = gid / half_size;
    uint pair = gid % half_size;
    uint i = group * full_size + pair;
    uint j = i + half_size;

    // For stages where the butterfly span (half_size) fits within the
    // threadgroup, we can use shared memory to avoid redundant global reads.
    // The condition: half_size <= tg_size means both i and j are within
    // the range of indices this threadgroup touches.

    if (half_size <= tg_size) {
        // Threadgroup-local butterfly via shared memory
        threadgroup float2 shared[FFT_TG_SIZE * 2];

        // Each thread loads both its butterfly elements
        // Map: within this threadgroup, threads cover a contiguous block of butterflies.
        // Thread tid handles butterfly pair (i, j).
        // We store in shared memory at positions based on local offset.
        uint local_i = tid;
        uint local_j = tid + tg_size;

        // But we need to map correctly: the two elements per thread
        // Load globally
        shared[local_i] = data[i];
        shared[local_j] = data[j];
        threadgroup_barrier(mem_flags::mem_threadgroup);

        // Compute twiddle and butterfly
        float2 w = twiddle(pair, full_size);
        float2 a = shared[local_i];
        float2 b = shared[local_j];
        float2 wb = cmul(w, b);

        shared[local_i] = a + wb;
        shared[local_j] = a - wb;
        threadgroup_barrier(mem_flags::mem_threadgroup);

        // Write back
        data[i] = shared[local_i];
        data[j] = shared[local_j];
    } else {
        // Large stage: global memory butterfly (span too large for shared mem)
        // Precompute twiddle in registers
        float2 w = twiddle(pair, full_size);
        float2 a = data[i];
        float2 b = data[j];
        float2 wb = cmul(w, b);
        data[i] = a + wb;
        data[j] = a - wb;
    }
}

// Radix-4 FFT with shared memory for data staging.
// Each radix-4 butterfly reads 4 elements, applies twiddle factors,
// and computes the 4-point DFT. Using shared memory to stage the reads
// improves coalescing when multiple threads in a group access nearby data.

kernel void fft_radix4_f32(
    device float2* data [[buffer(0)]],
    constant uint& N [[buffer(1)]],
    constant uint& stage [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]]
) {
    uint quarter = 1u << (stage * 2);
    uint block = quarter << 2;
    uint group = gid / quarter;
    uint pair = gid % quarter;
    uint base = group * block + pair;

    // Load 4 elements into registers
    float2 a0 = data[base];
    float2 a1 = data[base + quarter];
    float2 a2 = data[base + 2 * quarter];
    float2 a3 = data[base + 3 * quarter];

    // Precompute twiddle factors for positions 1, 2, 3
    // W_N^k for the radix-4 stage
    float2 w1 = twiddle(pair, block);
    float2 w2 = twiddle(2 * pair, block);
    float2 w3 = twiddle(3 * pair, block);

    // Apply twiddle factors to inputs 1, 2, 3
    a1 = cmul(w1, a1);
    a2 = cmul(w2, a2);
    a3 = cmul(w3, a3);

    // 4-point DFT butterfly (decimation-in-frequency)
    float2 t0 = a0 + a2;
    float2 t1 = a0 - a2;
    float2 t2 = a1 + a3;
    // -j * (a1 - a3): multiply by float2(0, -1) = swap and negate
    float2 diff13 = a1 - a3;
    float2 t3 = float2(diff13.y, -diff13.x);

    data[base]               = t0 + t2;
    data[base + quarter]     = t1 + t3;
    data[base + 2 * quarter] = t0 - t2;
    data[base + 3 * quarter] = t1 - t3;
}

// IFFT with shared-memory optimization for small stages (mirrors radix-2 forward).
kernel void ifft_f32(
    device float2* data [[buffer(0)]],
    constant uint& N [[buffer(1)]],
    constant uint& stage [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]]
) {
    uint half_size = 1u << stage;
    uint full_size = half_size << 1;
    uint group = gid / half_size;
    uint pair = gid % half_size;
    uint i = group * full_size + pair;
    uint j = i + half_size;

    if (half_size <= tg_size) {
        threadgroup float2 shared[FFT_TG_SIZE * 2];
        uint local_i = tid;
        uint local_j = tid + tg_size;

        shared[local_i] = data[i];
        shared[local_j] = data[j];
        threadgroup_barrier(mem_flags::mem_threadgroup);

        float2 w = twiddle_inv(pair, full_size);
        float2 a = shared[local_i];
        float2 b = shared[local_j];
        float2 wb = cmul(w, b);

        shared[local_i] = a + wb;
        shared[local_j] = a - wb;
        threadgroup_barrier(mem_flags::mem_threadgroup);

        data[i] = shared[local_i];
        data[j] = shared[local_j];
    } else {
        float2 w = twiddle_inv(pair, full_size);
        float2 a = data[i];
        float2 b = data[j];
        float2 wb = cmul(w, b);
        data[i] = a + wb;
        data[j] = a - wb;
    }
}

kernel void spectral_power_density_f32(
    device const float2* fft_data [[buffer(0)]],
    device float* output [[buffer(1)]],
    uint gid [[thread_position_in_grid]]
) {
    float2 c = fft_data[gid];
    output[gid] = fma(c.x, c.x, c.y * c.y);
}
"#;

pub const SIGNAL: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Conv1D with sliding window in threadgroup memory.
// Each threadgroup loads a tile of input (including halo for the kernel radius)
// into shared memory, so all threads share the loaded data.
// kernel_len is 32 in the benchmark; half = 16, so halo = 16 on each side.
// TG_SIZE is 32 (thread_execution_width). Tile covers TG_SIZE output positions.
// Shared memory: TG_SIZE + kernel_len - 1 elements (at most 32 + 63 = 95).

#define SIGNAL_TG_SIZE 32
#define SIGNAL_MAX_KERNEL 64
// Shared tile size: TG_SIZE + max halo on both sides
#define SIGNAL_TILE_SIZE (SIGNAL_TG_SIZE + SIGNAL_MAX_KERNEL)

kernel void conv1d_f32(
    device const float* input [[buffer(0)]],
    device const float* kernel_data [[buffer(1)]],
    device float* output [[buffer(2)]],
    constant uint& input_len [[buffer(3)]],
    constant uint& kernel_len [[buffer(4)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    uint tgid [[threadgroup_position_in_grid]]
) {
    // Load filter coefficients into threadgroup memory (shared across all threads)
    threadgroup float s_kernel[SIGNAL_MAX_KERNEL];
    for (uint i = tid; i < kernel_len && i < SIGNAL_MAX_KERNEL; i += tg_size) {
        s_kernel[i] = kernel_data[i];
    }

    // Load input tile with halo into shared memory
    threadgroup float s_input[SIGNAL_TILE_SIZE];

    uint half_k = kernel_len / 2;
    // The first output position for this threadgroup
    uint tile_start = tgid * tg_size;
    // The first input position we need (including left halo)
    int load_start = int(tile_start) - int(half_k);
    // Total elements to load: tg_size + kernel_len - 1
    uint load_count = tg_size + kernel_len - 1;

    // Cooperative load: each thread loads multiple elements
    for (uint i = tid; i < load_count; i += tg_size) {
        int src = load_start + int(i);
        s_input[i] = (src >= 0 && uint(src) < input_len) ? input[src] : 0.0f;
    }

    threadgroup_barrier(mem_flags::mem_threadgroup);

    // Compute convolution from shared memory
    if (gid < input_len) {
        float sum = 0.0f;
        // tid maps to position within the shared tile (offset by 0, the halo is already accounted for)
        // s_input[tid + k] corresponds to input[tile_start - half_k + tid + k]
        // We want: sum over k of input[gid - half_k + k] * kernel_data[k]
        // gid - half_k + k = tile_start + tid - half_k + k
        // In shared mem that's: s_input[tid + k]
        for (uint k = 0; k < kernel_len; k++) {
            sum = fma(s_input[tid + k], s_kernel[k], sum);
        }
        output[gid] = sum;
    }
}

kernel void window_apply_f32(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    constant uint& n [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    // Hanning window - precompute reciprocal to avoid repeated division
    float w = 0.5f * (1.0f - cos(2.0f * M_PI_F * float(gid) / float(n - 1)));
    output[gid] = input[gid] * w;
}

// FIR filter with sliding window in threadgroup memory.
// FIR is causal: output[i] = sum_{t=0}^{num_taps-1} input[i-t] * coeffs[t]
// We load a tile of input into shared memory including the left halo (num_taps-1 elements).

kernel void fir_filter_f32(
    device const float* input [[buffer(0)]],
    device const float* coeffs [[buffer(1)]],
    device float* output [[buffer(2)]],
    constant uint& input_len [[buffer(3)]],
    constant uint& num_taps [[buffer(4)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    uint tgid [[threadgroup_position_in_grid]]
) {
    // Load coefficients into threadgroup memory
    threadgroup float s_coeffs[SIGNAL_MAX_KERNEL];
    for (uint i = tid; i < num_taps && i < SIGNAL_MAX_KERNEL; i += tg_size) {
        s_coeffs[i] = coeffs[i];
    }

    // Load input tile with left halo into shared memory
    // For FIR: output[i] needs input[i], input[i-1], ..., input[i-(num_taps-1)]
    // So we need (num_taps - 1) elements of left halo.
    threadgroup float s_input[SIGNAL_TILE_SIZE];

    uint tile_start = tgid * tg_size;
    int load_start = int(tile_start) - int(num_taps - 1);
    uint load_count = tg_size + num_taps - 1;

    for (uint i = tid; i < load_count; i += tg_size) {
        int src = load_start + int(i);
        s_input[i] = (src >= 0 && uint(src) < input_len) ? input[src] : 0.0f;
    }

    threadgroup_barrier(mem_flags::mem_threadgroup);

    if (gid < input_len) {
        float sum = 0.0f;
        // s_input[num_taps - 1 + tid] = input[tile_start + tid] = input[gid]
        // s_input[num_taps - 1 + tid - t] = input[gid - t]
        uint base_idx = (num_taps - 1) + tid;
        for (uint t = 0; t < num_taps; t++) {
            sum = fma(s_input[base_idx - t], s_coeffs[t], sum);
        }
        output[gid] = sum;
    }
}
"#;

pub const LINALG: &str = r#"
#include <metal_stdlib>
using namespace metal;

kernel void spmv_csr_f32(
    device const float* values [[buffer(0)]],
    device const uint* col_idx [[buffer(1)]],
    device const uint* row_ptr [[buffer(2)]],
    device const float* x [[buffer(3)]],
    device float* y [[buffer(4)]],
    uint gid [[thread_position_in_grid]]
) {
    float sum = 0.0f;
    uint start = row_ptr[gid];
    uint end = row_ptr[gid + 1];
    for (uint j = start; j < end; j++) {
        sum += values[j] * x[col_idx[j]];
    }
    y[gid] = sum;
}

kernel void matvec_f32(
    device const float* A [[buffer(0)]],
    device const float* x [[buffer(1)]],
    device float* y [[buffer(2)]],
    constant uint& cols [[buffer(3)]],
    uint gid [[thread_position_in_grid]]
) {
    float sum = 0.0f;
    uint base = gid * cols;
    for (uint j = 0; j < cols; j++) {
        sum += A[base + j] * x[j];
    }
    y[gid] = sum;
}

kernel void outer_product_f32(
    device const float* a [[buffer(0)]],
    device const float* b [[buffer(1)]],
    device float* C [[buffer(2)]],
    constant uint& n [[buffer(3)]],
    uint2 gid [[thread_position_in_grid]]
) {
    if (gid.x < n && gid.y < n) {
        C[gid.x * n + gid.y] = a[gid.x] * b[gid.y];
    }
}

kernel void matmul_batched_f32(
    device const float* A [[buffer(0)]],
    device const float* B [[buffer(1)]],
    device float* C [[buffer(2)]],
    constant uint& M [[buffer(3)]],
    constant uint& N [[buffer(4)]],
    constant uint& K [[buffer(5)]],
    uint2 gid [[thread_position_in_grid]]
) {
    uint row = gid.x;
    uint col = gid.y;
    if (row >= M || col >= N) return;
    float sum = 0.0f;
    for (uint k = 0; k < K; k++) {
        sum += A[row * K + k] * B[k * N + col];
    }
    C[row * N + col] = sum;
}
"#;

pub const SIMULATION: &str = r#"
#include <metal_stdlib>
using namespace metal;

// 2D Heat equation with halo exchange in threadgroup memory.
// Dispatch: 2D grid [width, width], threadgroup 16x16.
// Each threadgroup loads an 18x18 tile (16x16 interior + 1-cell halo on each side)
// into shared memory. Interior threads read only from shared memory.
// Only edge threads of the threadgroup perform global reads for halos.

#define SIM_TG_W 16
#define SIM_TG_H 16
#define SIM_TILE_W (SIM_TG_W + 2)
#define SIM_TILE_H (SIM_TG_H + 2)

kernel void sim_heat_equation_f32(
    device const float* u [[buffer(0)]],
    device float* u_next [[buffer(1)]],
    constant uint& width [[buffer(2)]],
    constant float& alpha [[buffer(3)]],
    uint2 gid [[thread_position_in_grid]],
    uint2 tid [[thread_position_in_threadgroup]],
    uint2 tgid [[threadgroup_position_in_grid]]
) {
    // Shared memory tile with 1-cell halo
    threadgroup float tile[SIM_TILE_H][SIM_TILE_W];

    uint x = gid.x;
    uint y = gid.y;

    // Tile origin in global coords (top-left of the interior region)
    int tile_origin_x = int(tgid.x * SIM_TG_W);
    int tile_origin_y = int(tgid.y * SIM_TG_H);

    // Each thread loads one interior cell
    int gx = tile_origin_x + int(tid.x);
    int gy = tile_origin_y + int(tid.y);

    // Local coords within the tile (offset by 1 for halo)
    uint lx = tid.x + 1;
    uint ly = tid.y + 1;

    // Load interior cell
    float center = 0.0f;
    if (uint(gx) < width && uint(gy) < width) {
        center = u[uint(gy) * width + uint(gx)];
    }
    tile[ly][lx] = center;

    // Load halo cells: edge threads in the threadgroup load the bordering cells.
    // Top halo (row 0 of tile)
    if (tid.y == 0) {
        int hy = gy - 1;
        tile[0][lx] = (hy >= 0 && uint(gx) < width) ? u[uint(hy) * width + uint(gx)] : 0.0f;
    }
    // Bottom halo (row SIM_TG_H+1 of tile)
    if (tid.y == SIM_TG_H - 1) {
        int hy = gy + 1;
        tile[SIM_TG_H + 1][lx] = (uint(hy) < width && uint(gx) < width) ? u[uint(hy) * width + uint(gx)] : 0.0f;
    }
    // Left halo (col 0 of tile)
    if (tid.x == 0) {
        int hx = gx - 1;
        tile[ly][0] = (hx >= 0 && uint(gy) < width) ? u[uint(gy) * width + uint(hx)] : 0.0f;
    }
    // Right halo (col SIM_TG_W+1 of tile)
    if (tid.x == SIM_TG_W - 1) {
        int hx = gx + 1;
        tile[ly][SIM_TG_W + 1] = (uint(hx) < width && uint(gy) < width) ? u[uint(gy) * width + uint(hx)] : 0.0f;
    }
    // Corner halos
    if (tid.x == 0 && tid.y == 0) {
        int hx = gx - 1, hy = gy - 1;
        tile[0][0] = (hx >= 0 && hy >= 0) ? u[uint(hy) * width + uint(hx)] : 0.0f;
    }
    if (tid.x == SIM_TG_W - 1 && tid.y == 0) {
        int hx = gx + 1, hy = gy - 1;
        tile[0][SIM_TG_W + 1] = (uint(hx) < width && hy >= 0) ? u[uint(hy) * width + uint(hx)] : 0.0f;
    }
    if (tid.x == 0 && tid.y == SIM_TG_H - 1) {
        int hx = gx - 1, hy = gy + 1;
        tile[SIM_TG_H + 1][0] = (hx >= 0 && uint(hy) < width) ? u[uint(hy) * width + uint(hx)] : 0.0f;
    }
    if (tid.x == SIM_TG_W - 1 && tid.y == SIM_TG_H - 1) {
        int hx = gx + 1, hy = gy + 1;
        tile[SIM_TG_H + 1][SIM_TG_W + 1] = (uint(hx) < width && uint(hy) < width) ? u[uint(hy) * width + uint(hx)] : 0.0f;
    }

    threadgroup_barrier(mem_flags::mem_threadgroup);

    // Compute stencil from shared memory
    if (x >= width || y >= width) return;
    if (x == 0 || y == 0 || x >= width - 1 || y >= width - 1) {
        u_next[y * width + x] = center;
        return;
    }

    float left  = tile[ly][lx - 1];
    float right = tile[ly][lx + 1];
    float up    = tile[ly - 1][lx];
    float down  = tile[ly + 1][lx];
    u_next[y * width + x] = fma(alpha, left + right + up + down - 4.0f * center, center);
}

// 2D Wave equation with halo exchange in threadgroup memory.
// Same tiling strategy as heat equation but reads from two time steps.

kernel void sim_wave_equation_f32(
    device const float* u_curr [[buffer(0)]],
    device const float* u_prev [[buffer(1)]],
    device float* u_next [[buffer(2)]],
    constant uint& width [[buffer(3)]],
    constant float& c2 [[buffer(4)]],
    uint2 gid [[thread_position_in_grid]],
    uint2 tid [[thread_position_in_threadgroup]],
    uint2 tgid [[threadgroup_position_in_grid]]
) {
    threadgroup float tile_curr[SIM_TILE_H][SIM_TILE_W];

    uint x = gid.x;
    uint y = gid.y;

    int tile_origin_x = int(tgid.x * SIM_TG_W);
    int tile_origin_y = int(tgid.y * SIM_TG_H);
    int gx = tile_origin_x + int(tid.x);
    int gy = tile_origin_y + int(tid.y);
    uint lx = tid.x + 1;
    uint ly = tid.y + 1;

    // Load interior
    float center = 0.0f;
    if (uint(gx) < width && uint(gy) < width) {
        center = u_curr[uint(gy) * width + uint(gx)];
    }
    tile_curr[ly][lx] = center;

    // Load halos for u_curr
    if (tid.y == 0) {
        int hy = gy - 1;
        tile_curr[0][lx] = (hy >= 0 && uint(gx) < width) ? u_curr[uint(hy) * width + uint(gx)] : 0.0f;
    }
    if (tid.y == SIM_TG_H - 1) {
        int hy = gy + 1;
        tile_curr[SIM_TG_H + 1][lx] = (uint(hy) < width && uint(gx) < width) ? u_curr[uint(hy) * width + uint(gx)] : 0.0f;
    }
    if (tid.x == 0) {
        int hx = gx - 1;
        tile_curr[ly][0] = (hx >= 0 && uint(gy) < width) ? u_curr[uint(gy) * width + uint(hx)] : 0.0f;
    }
    if (tid.x == SIM_TG_W - 1) {
        int hx = gx + 1;
        tile_curr[ly][SIM_TG_W + 1] = (uint(hx) < width && uint(gy) < width) ? u_curr[uint(gy) * width + uint(hx)] : 0.0f;
    }
    // Corners
    if (tid.x == 0 && tid.y == 0) {
        int hx = gx - 1, hy = gy - 1;
        tile_curr[0][0] = (hx >= 0 && hy >= 0) ? u_curr[uint(hy) * width + uint(hx)] : 0.0f;
    }
    if (tid.x == SIM_TG_W - 1 && tid.y == 0) {
        int hx = gx + 1, hy = gy - 1;
        tile_curr[0][SIM_TG_W + 1] = (uint(hx) < width && hy >= 0) ? u_curr[uint(hy) * width + uint(hx)] : 0.0f;
    }
    if (tid.x == 0 && tid.y == SIM_TG_H - 1) {
        int hx = gx - 1, hy = gy + 1;
        tile_curr[SIM_TG_H + 1][0] = (hx >= 0 && uint(hy) < width) ? u_curr[uint(hy) * width + uint(hx)] : 0.0f;
    }
    if (tid.x == SIM_TG_W - 1 && tid.y == SIM_TG_H - 1) {
        int hx = gx + 1, hy = gy + 1;
        tile_curr[SIM_TG_H + 1][SIM_TG_W + 1] = (uint(hx) < width && uint(hy) < width) ? u_curr[uint(hy) * width + uint(hx)] : 0.0f;
    }

    threadgroup_barrier(mem_flags::mem_threadgroup);

    if (x >= width || y >= width) return;
    if (x == 0 || y == 0 || x >= width - 1 || y >= width - 1) {
        u_next[y * width + x] = 0.0f;
        return;
    }

    // Compute Laplacian from shared memory
    float left  = tile_curr[ly][lx - 1];
    float right = tile_curr[ly][lx + 1];
    float up    = tile_curr[ly - 1][lx];
    float down  = tile_curr[ly + 1][lx];
    float laplacian = left + right + up + down - 4.0f * center;

    // u_prev only needs the single point (no neighbors), read from global
    float prev = u_prev[y * width + x];
    u_next[y * width + x] = fma(c2, laplacian, 2.0f * center - prev);
}

// RK4 integrator: pure ALU, all computation in registers.
// Optimized by precomputing dt constants and using fma.
kernel void integrate_rk4_f32(
    device const float* y [[buffer(0)]],
    device float* y_next [[buffer(1)]],
    constant float& dt [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    float yi = y[gid];
    // f(y) = -y, so all k values are computed in registers.
    // Precompute constants to reduce dependent multiplies.
    float half_dt = 0.5f * dt;
    float dt_sixth = dt * (1.0f / 6.0f);

    float k1 = -yi;
    float k2 = -fma(half_dt, k1, yi);       // -(yi + 0.5*dt*k1)
    float k3 = -fma(half_dt, k2, yi);       // -(yi + 0.5*dt*k2)
    float k4 = -fma(dt, k3, yi);            // -(yi + dt*k3)

    // yi + (dt/6) * (k1 + 2*k2 + 2*k3 + k4)
    y_next[gid] = fma(dt_sixth, k1 + 2.0f * k2 + 2.0f * k3 + k4, yi);
}

// Velocity Verlet: pure register computation, precompute dt*dt.
kernel void integrate_velocity_verlet_f32(
    device float* pos [[buffer(0)]],
    device float* vel [[buffer(1)]],
    device const float* acc [[buffer(2)]],
    constant float& dt [[buffer(3)]],
    uint gid [[thread_position_in_grid]]
) {
    float p = pos[gid];
    float v = vel[gid];
    float a = acc[gid];
    float half_dt = 0.5f * dt;
    float dt2_half = half_dt * dt;

    p = fma(v, dt, fma(dt2_half, a, p));     // p + v*dt + 0.5*a*dt^2
    float a_new = -p;                         // simple harmonic: a = -x
    v = fma(half_dt, a + a_new, v);           // v + 0.5*(a + a_new)*dt
    pos[gid] = p;
    vel[gid] = v;
}

kernel void monte_carlo_integrate_f32(
    device const float* random_x [[buffer(0)]],
    device float* output [[buffer(1)]],
    uint gid [[thread_position_in_grid]]
) {
    float x = random_x[gid];
    output[gid] = 4.0f / fma(x, x, 1.0f); // pi estimation via integral of 4/(1+x^2)
}
"#;

pub const SORTING: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Bitonic sort with threadgroup-local compare-swap for small substages.
// When the butterfly span (half_block = 1 << substage) fits within the
// threadgroup, we load both elements into shared memory, do the compare-swap
// locally, and write back -- reducing global memory traffic.
// Threadgroup size = 32 (thread_execution_width on Apple Silicon).

#define SORT_TG_SIZE 32

kernel void sort_bitonic_f32(
    device float* data [[buffer(0)]],
    constant uint& stage [[buffer(1)]],
    constant uint& substage [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]]
) {
    uint half_block = 1u << substage;
    uint block = half_block << 1;
    uint group_id = gid / half_block;
    uint pair_id = gid % half_block;
    uint i = group_id * block + pair_id;
    uint j = i + half_block;
    bool ascending = ((i >> (stage + 1)) & 1) == 0;

    if (half_block <= tg_size) {
        // Small substage: use threadgroup memory
        threadgroup float shared[SORT_TG_SIZE * 2];

        // Load both elements into shared memory
        shared[tid] = data[i];
        shared[tid + tg_size] = data[j];
        threadgroup_barrier(mem_flags::mem_threadgroup);

        float a = shared[tid];
        float b = shared[tid + tg_size];

        // Compare-swap
        if (ascending ? (a > b) : (a < b)) {
            shared[tid] = b;
            shared[tid + tg_size] = a;
        }
        threadgroup_barrier(mem_flags::mem_threadgroup);

        // Write back
        data[i] = shared[tid];
        data[j] = shared[tid + tg_size];
    } else {
        // Large substage: global memory compare-swap
        float a = data[i];
        float b = data[j];
        if (ascending ? (a > b) : (a < b)) {
            data[i] = b;
            data[j] = a;
        }
    }
}

// Bitonic key-value sort with threadgroup-local stages.
kernel void sort_bitonic_kv_f32(
    device float* keys [[buffer(0)]],
    device uint* values [[buffer(1)]],
    constant uint& stage [[buffer(2)]],
    constant uint& substage [[buffer(3)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]]
) {
    uint half_block = 1u << substage;
    uint block = half_block << 1;
    uint group_id = gid / half_block;
    uint pair_id = gid % half_block;
    uint i = group_id * block + pair_id;
    uint j = i + half_block;
    bool ascending = ((i >> (stage + 1)) & 1) == 0;

    if (half_block <= tg_size) {
        // Small substage: use threadgroup memory for both keys and values
        threadgroup float s_keys[SORT_TG_SIZE * 2];
        threadgroup uint s_vals[SORT_TG_SIZE * 2];

        s_keys[tid] = keys[i];
        s_keys[tid + tg_size] = keys[j];
        s_vals[tid] = values[i];
        s_vals[tid + tg_size] = values[j];
        threadgroup_barrier(mem_flags::mem_threadgroup);

        float ka = s_keys[tid];
        float kb = s_keys[tid + tg_size];

        if (ascending ? (ka > kb) : (ka < kb)) {
            s_keys[tid] = kb;
            s_keys[tid + tg_size] = ka;
            uint va = s_vals[tid];
            uint vb = s_vals[tid + tg_size];
            s_vals[tid] = vb;
            s_vals[tid + tg_size] = va;
        }
        threadgroup_barrier(mem_flags::mem_threadgroup);

        keys[i] = s_keys[tid];
        keys[j] = s_keys[tid + tg_size];
        values[i] = s_vals[tid];
        values[j] = s_vals[tid + tg_size];
    } else {
        // Large substage: global memory
        float ka = keys[i], kb = keys[j];
        if (ascending ? (ka > kb) : (ka < kb)) {
            keys[i] = kb; keys[j] = ka;
            uint va = values[i], vb = values[j];
            values[i] = vb; values[j] = va;
        }
    }
}
"#;

pub const PRNG: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Philox 4x32-10 counter-based PRNG
uint4 philox4x32(uint4 ctr, uint2 key) {
    for (int i = 0; i < 10; i++) {
        uint lo0 = ctr.x * 0xD2511F53u;
        uint hi0 = mulhi(ctr.x, 0xD2511F53u);
        uint lo1 = ctr.z * 0xCD9E8D57u;
        uint hi1 = mulhi(ctr.z, 0xCD9E8D57u);
        ctr = uint4(hi1 ^ ctr.y ^ key.x, lo1, hi0 ^ ctr.w ^ key.y, lo0);
        key.x += 0x9E3779B9u;
        key.y += 0xBB67AE85u;
    }
    return ctr;
}

kernel void prng_philox_u32x4(
    device uint4* output [[buffer(0)]],
    constant uint2& key [[buffer(1)]],
    uint gid [[thread_position_in_grid]]
) {
    uint4 ctr = uint4(gid, 0u, 0u, 0u);
    output[gid] = philox4x32(ctr, key);
}

kernel void prng_uniform_f32(
    device float* output [[buffer(0)]],
    constant uint2& key [[buffer(1)]],
    uint gid [[thread_position_in_grid]]
) {
    uint4 ctr = uint4(gid, 0u, 0u, 0u);
    uint4 r = philox4x32(ctr, key);
    output[gid] = float(r.x) / float(0xFFFFFFFFu);
}

kernel void prng_normal_f32(
    device float* output [[buffer(0)]],
    constant uint2& key [[buffer(1)]],
    uint gid [[thread_position_in_grid]]
) {
    uint4 ctr = uint4(gid, 0u, 0u, 0u);
    uint4 r = philox4x32(ctr, key);
    float u1 = max(float(r.x) / float(0xFFFFFFFFu), 1e-7f);
    float u2 = float(r.y) / float(0xFFFFFFFFu);
    output[gid] = sqrt(-2.0f * log(u1)) * cos(2.0f * M_PI_F * u2);
}

kernel void prng_dropout_mask(
    device uchar* output [[buffer(0)]],
    constant uint2& key [[buffer(1)]],
    constant float& keep_prob [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    uint4 ctr = uint4(gid, 0u, 0u, 0u);
    uint4 r = philox4x32(ctr, key);
    output[gid] = (float(r.x) / float(0xFFFFFFFFu)) < keep_prob ? 1 : 0;
}
"#;

pub const LAYOUT: &str = r#"
#include <metal_stdlib>
using namespace metal;

// 16x16 tiled transpose with bank-conflict-free shared memory.
// tile[16][17]: the extra column (17 vs 16) eliminates bank conflicts
// when threads read columns from the transposed tile.
// Coalesced reads into tile rows, coalesced writes from tile columns.

kernel void transpose_2d(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    constant uint& rows [[buffer(2)]],
    constant uint& cols [[buffer(3)]],
    uint2 gid [[thread_position_in_grid]],
    uint2 tid [[thread_position_in_threadgroup]],
    uint2 tgid [[threadgroup_position_in_grid]]
) {
    // 17 columns to avoid bank conflicts on 32-bank shared memory
    threadgroup float tile[16][17];

    // Source coordinates: coalesced read along row
    uint src_r = tgid.x * 16 + tid.x;
    uint src_c = tgid.y * 16 + tid.y;

    // Load tile from input (coalesced: threads in a row read consecutive addresses)
    if (src_r < rows && src_c < cols) {
        tile[tid.x][tid.y] = input[src_r * cols + src_c];
    }
    threadgroup_barrier(mem_flags::mem_threadgroup);

    // Destination coordinates: swapped tile indices for transposed write
    uint dst_r = tgid.y * 16 + tid.x;
    uint dst_c = tgid.x * 16 + tid.y;

    // Write transposed tile (coalesced: threads in a row write consecutive addresses)
    if (dst_r < cols && dst_c < rows) {
        output[dst_r * rows + dst_c] = tile[tid.y][tid.x];
    }
}

kernel void repack_aos_to_soa(
    device const float4* aos [[buffer(0)]],
    device float* soa_x [[buffer(1)]],
    device float* soa_y [[buffer(2)]],
    device float* soa_z [[buffer(3)]],
    device float* soa_w [[buffer(4)]],
    uint gid [[thread_position_in_grid]]
) {
    float4 v = aos[gid];
    soa_x[gid] = v.x;
    soa_y[gid] = v.y;
    soa_z[gid] = v.z;
    soa_w[gid] = v.w;
}

kernel void repack_soa_to_aos(
    device const float* soa_x [[buffer(0)]],
    device const float* soa_y [[buffer(1)]],
    device const float* soa_z [[buffer(2)]],
    device const float* soa_w [[buffer(3)]],
    device float4* aos [[buffer(4)]],
    uint gid [[thread_position_in_grid]]
) {
    aos[gid] = float4(soa_x[gid], soa_y[gid], soa_z[gid], soa_w[gid]);
}
"#;

pub const SCANS: &str = r#"
#include <metal_stdlib>
using namespace metal;

kernel void scan_exclusive_u32(
    device uint* data [[buffer(0)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup uint* shared [[threadgroup(0)]]
) {
    shared[tid] = data[gid];
    threadgroup_barrier(mem_flags::mem_threadgroup);
    // Up-sweep
    for (uint d = 1; d < tg_size; d <<= 1) {
        uint ai = (tid + 1) * (d << 1) - 1;
        if (ai < tg_size) shared[ai] += shared[ai - d];
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }
    if (tid == tg_size - 1) shared[tid] = 0;
    threadgroup_barrier(mem_flags::mem_threadgroup);
    // Down-sweep
    for (uint d = tg_size >> 1; d > 0; d >>= 1) {
        uint ai = (tid + 1) * (d << 1) - 1;
        if (ai < tg_size) {
            uint temp = shared[ai];
            shared[ai] += shared[ai - d];
            shared[ai - d] = temp;
        }
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }
    data[gid] = shared[tid];
}

kernel void scan_inclusive_u32(
    device uint* data [[buffer(0)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup uint* shared [[threadgroup(0)]]
) {
    shared[tid] = data[gid];
    threadgroup_barrier(mem_flags::mem_threadgroup);
    for (uint d = 1; d < tg_size; d <<= 1) {
        uint val = (tid >= d) ? shared[tid - d] : 0;
        threadgroup_barrier(mem_flags::mem_threadgroup);
        shared[tid] += val;
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }
    data[gid] = shared[tid];
}

kernel void compact_mask(
    device const float* input [[buffer(0)]],
    device const uint* mask [[buffer(1)]],
    device const uint* scan [[buffer(2)]],
    device float* output [[buffer(3)]],
    uint gid [[thread_position_in_grid]]
) {
    if (mask[gid]) {
        output[scan[gid]] = input[gid];
    }
}

kernel void compact_if(
    device const float* input [[buffer(0)]],
    device const uint* scan [[buffer(1)]],
    device float* output [[buffer(2)]],
    constant float& threshold [[buffer(3)]],
    uint gid [[thread_position_in_grid]]
) {
    if (input[gid] > threshold) {
        output[scan[gid]] = input[gid];
    }
}
"#;

pub const QUANTIZATION: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Vectorized quantization: each thread converts 4 elements at once.
// float4 -> half4, half4 -> char4, char4 -> half4 for 4x throughput.

kernel void quantize_f32_to_f16(
    device const float* input [[buffer(0)]],
    device half* output [[buffer(1)]],
    uint gid [[thread_position_in_grid]]
) {
    float4 v = ((device const float4*)input)[gid];
    ((device half4*)output)[gid] = half4(v);
}

kernel void quantize_f16_to_i8(
    device const half* input [[buffer(0)]],
    device char* output [[buffer(1)]],
    constant half& scale [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    half4 v = ((device const half4*)input)[gid];
    float4 scaled = float4(v) / float(scale);
    float4 clamped = clamp(round(scaled), -127.0f, 127.0f);
    ((device char4*)output)[gid] = char4(clamped);
}

kernel void dequantize_i8_to_f16(
    device const char* input [[buffer(0)]],
    device half* output [[buffer(1)]],
    constant half& scale [[buffer(2)]],
    uint gid [[thread_position_in_grid]]
) {
    char4 v = ((device const char4*)input)[gid];
    float4 result = float4(v.x, v.y, v.z, v.w) * float(scale);
    ((device half4*)output)[gid] = half4(result);
}
"#;

pub const DOT_PRODUCTS: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Vectorized dot products: float4 inner loops for 4x memory throughput.

kernel void dot_batched_f32(
    device const float* a [[buffer(0)]],
    device const float* b [[buffer(1)]],
    device float* output [[buffer(2)]],
    constant uint& dim [[buffer(3)]],
    uint gid [[thread_position_in_grid]]
) {
    uint base = gid * dim;
    device const float4* a4 = (device const float4*)(a + base);
    device const float4* b4 = (device const float4*)(b + base);
    float4 acc = float4(0.0f);
    uint d4 = dim / 4;
    for (uint i = 0; i < d4; i++) {
        acc = fma(a4[i], b4[i], acc);
    }
    float sum = acc.x + acc.y + acc.z + acc.w;
    // Handle remainder
    for (uint d = d4 * 4; d < dim; d++) {
        sum = fma(a[base + d], b[base + d], sum);
    }
    output[gid] = sum;
}

kernel void cosine_similarity_batched(
    device const float* a [[buffer(0)]],
    device const float* b [[buffer(1)]],
    device float* output [[buffer(2)]],
    constant uint& dim [[buffer(3)]],
    uint gid [[thread_position_in_grid]]
) {
    uint base = gid * dim;
    device const float4* a4 = (device const float4*)(a + base);
    device const float4* b4 = (device const float4*)(b + base);
    float4 dot_acc = float4(0.0f);
    float4 na_acc = float4(0.0f);
    float4 nb_acc = float4(0.0f);
    uint d4 = dim / 4;
    for (uint i = 0; i < d4; i++) {
        float4 ai = a4[i], bi = b4[i];
        dot_acc = fma(ai, bi, dot_acc);
        na_acc = fma(ai, ai, na_acc);
        nb_acc = fma(bi, bi, nb_acc);
    }
    float dot = dot_acc.x + dot_acc.y + dot_acc.z + dot_acc.w;
    float norm_a = na_acc.x + na_acc.y + na_acc.z + na_acc.w;
    float norm_b = nb_acc.x + nb_acc.y + nb_acc.z + nb_acc.w;
    for (uint d = d4 * 4; d < dim; d++) {
        float ai = a[base + d], bi = b[base + d];
        dot = fma(ai, bi, dot);
        norm_a = fma(ai, ai, norm_a);
        norm_b = fma(bi, bi, norm_b);
    }
    output[gid] = dot / (sqrt(norm_a) * sqrt(norm_b) + 1e-8f);
}
"#;

pub const FUSED: &str = r#"
#include <metal_stdlib>
using namespace metal;

// Fused map-reduce: map computed in registers, then simd_sum for reduction.
// No shared memory or barriers needed -- each threadgroup = one simdgroup (32 threads).

kernel void map_reduce_square_sum(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float v = input[gid];
    float sum = simd_sum(v * v);
    if (tid == 0) output[gid / tg_size] = sum;
}

kernel void map_reduce_abs_sum(
    device const float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float sum = simd_sum(abs(input[gid]));
    if (tid == 0) output[gid / tg_size] = sum;
}

kernel void map_reduce_masked_sum(
    device const float* input [[buffer(0)]],
    device const float* mask [[buffer(1)]],
    device float* output [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float sum = simd_sum(input[gid] * mask[gid]);
    if (tid == 0) output[gid / tg_size] = sum;
}

kernel void map_reduce_threshold_count(
    device const float* input [[buffer(0)]],
    device uint* output [[buffer(1)]],
    constant float& threshold [[buffer(2)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup uint* shared [[threadgroup(0)]]
) {
    uint val = (input[gid] > threshold) ? 1u : 0u;
    uint sum = simd_sum(val);
    if (tid == 0) output[gid / tg_size] = sum;
}
"#;

pub const FUSED_CUSTOM: &str = r#"
#include <metal_stdlib>
using namespace metal;

// 
// 1. Fused LayerNorm + Dropout + Residual Add
//    Three memory passes collapsed into one:
//      pass 1: compute mean/var (reduction over row)
//      pass 2: normalize, apply dropout mask, add residual
//    MLX's graph compiler cannot fuse the dropout mask application
//    with the layernorm reduction because the mask introduces a
//    data-dependent branch inside the reduction body.
// 
kernel void fused_layernorm_dropout_residual(
    device const float* input      [[buffer(0)]],
    device const float* residual   [[buffer(1)]],
    device const float* gamma      [[buffer(2)]],
    device const float* beta       [[buffer(3)]],
    device const uchar* drop_mask  [[buffer(4)]],
    device float* output           [[buffer(5)]],
    constant uint& dim             [[buffer(6)]],
    constant float& drop_scale     [[buffer(7)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    float val = input[gid];

    //  Mean reduction via simd_sum 
    float mean = simd_sum(val) / float(dim);

    //  Variance reduction via simd_sum 
    float diff = val - mean;
    float var = simd_sum(diff * diff) / float(dim);
    float inv_std = rsqrt(var + 1e-5f);

    //  Normalize + dropout + residual in one write 
    uint col = gid % dim;
    float normed = fma(gamma[col], diff * inv_std, beta[col]);
    float dropped = normed * float(drop_mask[gid]) * drop_scale;
    output[gid] = dropped + residual[gid];
}

// 
// 2. Fused Tiled Attention: QK^T -> online softmax -> V multiply
//    Single-pass Flash-Attention style: never materializes the NxN
//    attention matrix. Uses online softmax (Milakov-Gimelshein) to
//    compute numerically stable softmax in a streaming fashion.
//    MLX's mx.fast.scaled_dot_product_attention exists but this
//    custom kernel can be tuned for different tile sizes and
//    head dimensions.
// 
kernel void fused_attention_softmax_v(
    device const float* Q     [[buffer(0)]],
    device const float* K     [[buffer(1)]],
    device const float* V     [[buffer(2)]],
    device float* output      [[buffer(3)]],
    constant uint& seq_len    [[buffer(4)]],
    constant uint& head_dim   [[buffer(5)]],
    uint2 gid [[thread_position_in_grid]]
) {
    uint row = gid.x;  // query position
    uint col = gid.y;  // output head dimension
    if (row >= seq_len || col >= head_dim) return;

    float scale = rsqrt(float(head_dim));

    // Online softmax state
    float running_max = -INFINITY;
    float running_sum = 0.0f;
    float acc = 0.0f;

    for (uint k = 0; k < seq_len; k++) {
        // Compute QK^T dot product for this (row, k) pair
        float qk = 0.0f;
        for (uint d = 0; d < head_dim; d++) {
            qk += Q[row * head_dim + d] * K[k * head_dim + d];
        }
        qk *= scale;

        // Online softmax update (Milakov-Gimelshein algorithm)
        float new_max = max(running_max, qk);
        float correction = exp(running_max - new_max);
        float p = exp(qk - new_max);

        // Rescale accumulated output and running sum
        acc = acc * correction + p * V[k * head_dim + col];
        running_sum = running_sum * correction + p;
        running_max = new_max;
    }

    output[row * head_dim + col] = acc / running_sum;
}

// 
// 3. Fused Exclusive Prefix Scan + Stream Compaction
//    Normally requires 2 dispatches: (1) prefix scan over predicate,
//    (2) scatter using scan results. This fuses both into a single
//    dispatch using shared memory to pipeline scan then compact
//    within each threadgroup.
// 
kernel void fused_scan_compact(
    device const float* input     [[buffer(0)]],
    device float* output          [[buffer(1)]],
    device atomic_uint* out_count [[buffer(2)]],
    constant float& threshold     [[buffer(3)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup uint* s_pred [[threadgroup(0)]]
) {
    // Step 1: Evaluate predicate
    float val = input[gid];
    uint pred = (val > threshold) ? 1u : 0u;
    s_pred[tid] = pred;
    threadgroup_barrier(mem_flags::mem_threadgroup);

    // Step 2: Exclusive prefix scan (Blelloch) within threadgroup
    // Up-sweep
    for (uint d = 1; d < tg_size; d <<= 1) {
        uint ai = (tid + 1) * (d << 1) - 1;
        if (ai < tg_size) s_pred[ai] += s_pred[ai - d];
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }

    uint total_in_group = s_pred[tg_size - 1];
    if (tid == tg_size - 1) s_pred[tid] = 0;
    threadgroup_barrier(mem_flags::mem_threadgroup);

    // Down-sweep
    for (uint d = tg_size >> 1; d > 0; d >>= 1) {
        uint ai = (tid + 1) * (d << 1) - 1;
        if (ai < tg_size) {
            uint temp = s_pred[ai];
            s_pred[ai] += s_pred[ai - d];
            s_pred[ai - d] = temp;
        }
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }

    // Step 3: Get global output base via atomic add (one per threadgroup)
    threadgroup uint group_base;
    if (tid == 0) {
        group_base = atomic_fetch_add_explicit(out_count, total_in_group, memory_order_relaxed);
    }
    threadgroup_barrier(mem_flags::mem_threadgroup);

    // Step 4: Scatter compacted elements
    if (pred) {
        output[group_base + s_pred[tid]] = val;
    }
}

// 
// 4. Fused RoPE + Causal Mask + Scale
//    Applies rotary position embedding, causal attention mask, and
//    query scaling in a single pass over Q/K. Normally these are
//    3 separate operations: (1) RoPE rotation on Q and K,
//    (2) causal mask creation, (3) scale application.
//    MLX cannot fuse RoPE's paired rotation with masking because
//    RoPE requires reading two elements and writing two outputs
//    with trigonometric dependency, which breaks element-wise fusion.
// 
kernel void fused_rope_attention_mask(
    device float* Q              [[buffer(0)]],
    device float* K              [[buffer(1)]],
    device const float* cos_tab  [[buffer(2)]],
    device const float* sin_tab  [[buffer(3)]],
    device float* attn_out       [[buffer(4)]],
    constant uint& seq_len       [[buffer(5)]],
    constant uint& head_dim      [[buffer(6)]],
    uint2 gid [[thread_position_in_grid]]
) {
    uint q_pos = gid.x;
    uint k_pos = gid.y;
    if (q_pos >= seq_len || k_pos >= seq_len) return;

    float scale = rsqrt(float(head_dim));
    uint half_dim = head_dim / 2;

    // Apply RoPE to Q[q_pos] and K[k_pos] on-the-fly, compute dot product
    float dot = 0.0f;
    for (uint d = 0; d < half_dim; d++) {
        float cq = cos_tab[q_pos * half_dim + d];
        float sq = sin_tab[q_pos * half_dim + d];
        float ck = cos_tab[k_pos * half_dim + d];
        float sk = sin_tab[k_pos * half_dim + d];

        float q0 = Q[q_pos * head_dim + d];
        float q1 = Q[q_pos * head_dim + d + half_dim];
        float rotated_q0 = q0 * cq - q1 * sq;
        float rotated_q1 = q0 * sq + q1 * cq;

        float k0 = K[k_pos * head_dim + d];
        float k1 = K[k_pos * head_dim + d + half_dim];
        float rotated_k0 = k0 * ck - k1 * sk;
        float rotated_k1 = k0 * sk + k1 * ck;

        dot += rotated_q0 * rotated_k0 + rotated_q1 * rotated_k1;
    }

    // Apply causal mask: positions where k_pos > q_pos get -inf
    float masked = (k_pos > q_pos) ? -INFINITY : dot * scale;
    attn_out[q_pos * seq_len + k_pos] = masked;
}

// 
// 5. Fused Adam + Gradient Clipping + Weight Decay
//    Combines: (1) gradient clipping by value, (2) AdamW weight
//    decay, (3) Adam moment updates, (4) parameter update.
//    In MLX (and PyTorch) these are 3-4 separate dispatches.
//    Fusing saves 3 round-trips over parameter memory.
// 
kernel void fused_adam_clip_update(
    device float* params       [[buffer(0)]],
    device const float* grads  [[buffer(1)]],
    device float* m_buf        [[buffer(2)]],
    device float* v_buf        [[buffer(3)]],
    constant float& lr         [[buffer(4)]],
    constant float& beta1      [[buffer(5)]],
    constant float& beta2      [[buffer(6)]],
    constant float& eps        [[buffer(7)]],
    constant float& weight_decay [[buffer(8)]],
    constant float& clip_val   [[buffer(9)]],
    uint gid [[thread_position_in_grid]]
) {
    // Vectorized: 4 elements per thread
    // Step 1: Clip gradient by value
    float4 g = clamp(((device const float4*)grads)[gid], float4(-clip_val), float4(clip_val));

    // Step 2: Weight decay (AdamW-style: decay before moment update)
    float4 p = ((device float4*)params)[gid];
    p -= lr * weight_decay * p;

    // Step 3: Adam moment updates
    float4 mi = fma(float4(beta1), ((device float4*)m_buf)[gid], (1.0f - beta1) * g);
    float4 vi = fma(float4(beta2), ((device float4*)v_buf)[gid], (1.0f - beta2) * g * g);
    ((device float4*)m_buf)[gid] = mi;
    ((device float4*)v_buf)[gid] = vi;

    // Step 4: Parameter update
    ((device float4*)params)[gid] = p - lr * mi / (sqrt(vi) + eps);
}

// 
// 6. Fused Softmax Cross-Entropy (forward + backward)
//    Computes: log-softmax, NLL loss, AND the backward gradient
//    (softmax(logits) - one_hot(target)) in a single pass.
//    This is the training loop hotspot. Normally requires:
//    (1) softmax forward, (2) NLL computation, (3) gradient of
//    log-softmax w.r.t. logits  3 passes over the logit matrix.
// 
kernel void fused_softmax_cross_entropy(
    device const float* logits   [[buffer(0)]],
    device const uint*  targets  [[buffer(1)]],
    device float* loss_out       [[buffer(2)]],
    device float* grad_out       [[buffer(3)]],
    constant uint& num_classes   [[buffer(4)]],
    uint gid [[thread_position_in_grid]],
    uint tid [[thread_position_in_threadgroup]],
    uint tg_size [[threads_per_threadgroup]],
    threadgroup float* shared [[threadgroup(0)]]
) {
    // Each threadgroup handles one sample (row of logits)
    uint sample = gid / num_classes;
    uint c = gid % num_classes;
    uint base = sample * num_classes;
    float val = logits[base + c];

    //  Find max via simd_max (no shared mem) 
    float max_val = simd_max(val);

    //  Exp and sum via simd_sum 
    float e = exp(val - max_val);
    float sum_exp = simd_sum(e);

    //  Forward: softmax probability and NLL loss 
    float prob = e / sum_exp;
    uint target_class = targets[sample];
    if (c == target_class) {
        loss_out[sample] = -(val - max_val - log(sum_exp));
    }

    //  Backward: gradient = softmax(logit) - one_hot(target) 
    float one_hot = (c == target_class) ? 1.0f : 0.0f;
    grad_out[base + c] = prob - one_hot;
}
"#;
